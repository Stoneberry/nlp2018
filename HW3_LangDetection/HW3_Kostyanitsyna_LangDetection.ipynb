{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анастасия Костяницына\n",
    "\n",
    "### БКЛ-151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from string import punctuation, digits\n",
    "\n",
    "punctuation = set(punctuation + '«»—…“”\\n\\t' + digits)\n",
    "table = str.maketrans({ch: None for ch in punctuation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разработка алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10): # функция для скачивания статей из википедии\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cкачиваем по 100 статей для каждого языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2lang = wikipedia.languages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в code2lang всего 432 языка, этот процесс занимает очень много времени, поэтому ради эксперемента будем использовать лишь некоторые. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['ru', 'mk', 'bg', 'ky', 'en', 'mn', 'fr', 'be', 'uk', 'lez', 'mhr', 'kk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Иммануэль\n",
      "Skipping page Ткачук, Михаил\n",
      "Skipping page Философов\n",
      "Skipping page Улица Ильича\n",
      "Skipping page Извольский\n",
      "Skipping page Межиречка\n",
      "Skipping page Мухинский сельсовет\n",
      "Skipping page Константиновская волость\n",
      "Skipping page Рёч\n",
      "Skipping page Озолс\n",
      "ru 140\n",
      "mk 150\n",
      "Skipping page Трентън (пояснение)\n",
      "Skipping page Полемон\n",
      "Skipping page Обсада на Никея\n",
      "Skipping page Иван Стоянов\n",
      "Skipping page Кирил Стоянов (пояснение)\n",
      "Skipping page Брей\n",
      "bg 144\n",
      "ky 150\n",
      "Skipping page Dema\n",
      "Skipping page Bridal Veil Falls (Banff)\n",
      "Skipping page Bishop Smith\n",
      "Skipping page All Things to All Men (song)\n",
      "Skipping page Charles H. Jones\n",
      "Skipping page CCGA\n",
      "Skipping page Darj-e Sofla\n",
      "en 143\n",
      "mn 150\n",
      "Skipping page Centre de contrôle\n",
      "Skipping page Felici\n",
      "Skipping page Callahan\n",
      "Skipping page Église Saint-Jean-l'Évangéliste de Taulis\n",
      "fr 146\n",
      "Skipping page Мазкі\n",
      "Skipping page Аптычная вось\n",
      "Skipping page Мелхія (значэнні)\n",
      "Skipping page Багатыр\n",
      "Skipping page Падлужжа (Глускі раён)\n",
      "Skipping page Благавешчанск (значэнні)\n",
      "Skipping page Грышын\n",
      "Skipping page Майскае\n",
      "Skipping page Сакалы\n",
      "Skipping page Брыцікі\n",
      "Skipping page Андрэй Іванавіч Яроменка\n",
      "Skipping page Войтаўка\n",
      "be 138\n",
      "Skipping page Адам (значення)\n",
      "Skipping page Черокі (значення)\n",
      "Skipping page Обструкція\n",
      "Skipping page Монфор\n",
      "Skipping page Кнєжевес\n",
      "Skipping page Вулиця Сулеймана Стальського\n",
      "Skipping page Огуз (значення)\n",
      "Skipping page Нікітіна\n",
      "Skipping page Йиґісоо\n",
      "Skipping page Сафіна\n",
      "Skipping page Сандерсон\n",
      "Skipping page Медіна (округ)\n",
      "Skipping page Плака (значення)\n",
      "uk 137\n",
      "Skipping page Ванашимахи (манаяр)\n",
      "Skipping page Чилегир\n",
      "lez 148\n",
      "Skipping page Куфтино\n",
      "Skipping page Нольо\n",
      "Skipping page Покровск\n",
      "Skipping page Сосновка\n",
      "Skipping page Ӧрша (ыҥ-влак)\n",
      "Skipping page Эҥермучаш\n",
      "Skipping page Сидорово\n",
      "Skipping page Лидывуй\n",
      "mhr 142\n",
      "kk 150\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "\n",
    "for lang in langs:\n",
    "    try:\n",
    "        wiki_texts[lang] = get_texts_for_lang(lang, 150)\n",
    "    except Exception as e:\n",
    "        print('ERROR ON - ', lang, e)\n",
    "        continue\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Беньямин\n",
      "Skipping page Бартошек (значения)\n",
      "Skipping page Мысовское (сельское поселение)\n",
      "Skipping page Лейбман\n",
      "Skipping page Кочегуровка\n",
      "ru 95\n",
      "mk 100\n",
      "Skipping page Жидов гроб\n",
      "Skipping page Ломница\n",
      "bg 98\n",
      "ky 100\n",
      "Skipping page L. nitida\n",
      "Skipping page Gaber\n",
      "Skipping page Is That So?\n",
      "Skipping page Wilson Charles\n",
      "Skipping page This Is Your Sword\n",
      "Skipping page Sogpelcé\n",
      "Skipping page Peter Hayman\n",
      "en 93\n",
      "mn 100\n",
      "Skipping page Église Saint-Louis de Saint-Louis-lès-Bitche\n",
      "Skipping page Kelmendi\n",
      "Skipping page I Got You\n",
      "Skipping page Ladino\n",
      "Skipping page Anthony Powell\n",
      "Skipping page Marakwet\n",
      "Skipping page Sadovo\n",
      "fr 93\n",
      "Skipping page Зачэпічы\n",
      "Skipping page Нільс (значэнні)\n",
      "Skipping page Рыгор Сцяпанавіч Пірагоў\n",
      "Skipping page Рэдзькі\n",
      "be 96\n",
      "Skipping page Помбал\n",
      "Skipping page Верби\n",
      "Skipping page Віана\n",
      "Skipping page Лисун\n",
      "uk 96\n",
      "lez 100\n",
      "Skipping page Рвезылык (ыҥ-влак)\n",
      "Skipping page Пыжанъю (ыҥ-влак)\n",
      "Skipping page Квадрат (ыҥ-влак)\n",
      "Skipping page Ялнер\n",
      "mhr 96\n",
      "kk 100\n"
     ]
    }
   ],
   "source": [
    "test_texts = {}\n",
    "\n",
    "for lang in langs:\n",
    "    try:\n",
    "        test_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    except Exception as e:\n",
    "        print('ERROR ON - ', lang, e)\n",
    "        continue\n",
    "    print(lang, len(test_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование частотного списка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_words(wiki_texts, lang):\n",
    "    \n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    length = 0\n",
    "\n",
    "    try:\n",
    "        \n",
    "        corpus = wiki_texts[lang]\n",
    "        \n",
    "        for article in corpus:\n",
    "            \n",
    "            words = tokenize(article.replace('\\n', '').lower())\n",
    "            length += len(words)\n",
    "            \n",
    "            for word in words:\n",
    "         \n",
    "                freqs[word] += 1\n",
    "\n",
    "        return freqs, length\n",
    "            \n",
    "    except:\n",
    "        return freqs, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text).replace('\\n', '')\n",
    "    text = re.sub(r'[\\s]{2,}',' ',text)\n",
    "    return text.split(' ')\n",
    "\n",
    "\n",
    "word_langs = defaultdict(dict)\n",
    "lang_word = defaultdict(set)\n",
    "corpus_length = {}\n",
    "\n",
    "\n",
    "for lang in wiki_texts:\n",
    "    \n",
    "    freqs, length = key_words(wiki_texts, lang)\n",
    "    \n",
    "    a = defaultdict(set)\n",
    "\n",
    "    for word in freqs:\n",
    "        \n",
    "        lang_word[lang].add(word)\n",
    "        word_langs[word][lang] = freqs[word]\n",
    "   \n",
    "    corpus_length[lang] = length\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_langs: ключ - слово, значение - словарь для каждого языка, в котором встретилось слово и частотностью его употребления в тексте на этом языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': 24253,\n",
       " 'bg': 41927,\n",
       " 'en': 74212,\n",
       " 'fr': 72362,\n",
       " 'kk': 15648,\n",
       " 'ky': 22409,\n",
       " 'lez': 21595,\n",
       " 'mhr': 16090,\n",
       " 'mk': 44745,\n",
       " 'mn': 34273,\n",
       " 'ru': 75300,\n",
       " 'uk': 37334}"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем вероятности слов для кажого языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for word in word_langs:\n",
    "    \n",
    "    for lang in word_langs[word]:\n",
    "        \n",
    "        word_langs[word][lang] = word_langs[word][lang] / corpus_length[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be': 8.246402506906362e-05,\n",
       " 'bg': 0.0005485725189019009,\n",
       " 'kk': 6.390593047034765e-05,\n",
       " 'ky': 4.462492748449284e-05,\n",
       " 'lez': 0.015790692289881916,\n",
       " 'mhr': 0.00018645121193287757,\n",
       " 'ru': 0.00021248339973439575,\n",
       " 'uk': 0.00034820806771307654}"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_langs['я']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(text, lang_word, word_langs):\n",
    "    \n",
    "    punctuation = '(\\.|,|\\?|!|\\(|\\)|\\*|\\'|\\\"|:|;|>|<|/|—|»|«|=|\\{|\\}|\\[|\\]|\\-|_|\\+|\\&|\\*|\\^|\\%|\\$|@|\\#|”)'\n",
    "    text = re.sub(punctuation, ' ', text)\n",
    "    words = set(tokenize(text.replace('\\n', ',').lower()))\n",
    "    \n",
    "    lang_pred = defaultdict(lambda: 0)\n",
    "    \n",
    "    for lang in lang_word:\n",
    "\n",
    "        intersect = words & lang_word[lang]\n",
    "        \n",
    "        for word in intersect:\n",
    "            \n",
    "            lang_pred[lang] += word_langs[word][lang]     \n",
    "\n",
    "    return max(lang_pred.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mk'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_language('что это: за! язык такой?', lang_word, word_langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in test_texts:\n",
    "    for text in test_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_language(text, lang_word,  word_langs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       0.99      0.92      0.95        96\n",
      "         bg       0.66      1.00      0.79        98\n",
      "         en       0.79      1.00      0.88        93\n",
      "         fr       0.96      1.00      0.98        93\n",
      "         kk       0.98      0.94      0.96       100\n",
      "         ky       1.00      0.90      0.95       100\n",
      "        lez       0.99      0.86      0.92       100\n",
      "        mhr       0.94      0.98      0.96        96\n",
      "         mk       0.90      1.00      0.95       100\n",
      "         mn       1.00      0.94      0.97       100\n",
      "         ru       0.85      0.56      0.68        95\n",
      "         uk       1.00      0.77      0.87        96\n",
      "\n",
      "avg / total       0.92      0.91      0.91      1167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 88   2   3   1   0   0   0   0   1   0   1   0]\n",
      " [  0  98   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0  93   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  93   0   0   0   0   0   0   0   0]\n",
      " [  1   1   1   1  94   0   0   0   1   0   1   0]\n",
      " [  0   2   2   0   1  90   1   2   1   0   1   0]\n",
      " [  0   0   7   1   0   0  86   3   0   0   3   0]\n",
      " [  0   0   2   0   0   0   0  94   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 100   0   0   0]\n",
      " [  0   0   4   0   1   0   0   1   0  94   0   0]\n",
      " [  0  33   4   1   0   0   0   0   4   0  53   0]\n",
      " [  0  13   2   0   0   0   0   0   4   0   3  74]]\n"
     ]
    }
   ],
   "source": [
    "labels = list(set(lang_word))\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка списка частотных ngram для каждого языка "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_count(wiki_texts, lang):\n",
    "    \n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        corpus = wiki_texts[lang]\n",
    "        \n",
    "        for article in corpus:\n",
    "            \n",
    "            article = re.sub(r'[^\\w\\s]', '', article).replace('\\n', '')\n",
    "            article = re.sub(r'[\\s]{2,}', ' ', article)\n",
    "            \n",
    "            for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "                \n",
    "                freqs[ngram] += 1\n",
    "        \n",
    "        freqs = sorted(freqs, key=lambda n: freqs[n], reverse=True)[:300]\n",
    "        return freqs\n",
    "        \n",
    "    except:\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_ngram = defaultdict(list)\n",
    "\n",
    "for lang in wiki_texts:\n",
    "    \n",
    "    freqs = ngram_count(wiki_texts, lang)\n",
    "    all_ngram[lang] = freqs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция расстояния для двух веторов ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    \n",
    "    distance = 0\n",
    "    x = {}\n",
    "    y = {}\n",
    "    \n",
    "    for index, item in enumerate(a):\n",
    "        \n",
    "        x[item] = index\n",
    "\n",
    "    for index, item in enumerate(b):\n",
    "        \n",
    "        y[item] = index\n",
    "        \n",
    "    for item in a:\n",
    "        \n",
    "        if item in y:\n",
    "            \n",
    "            if x[item] != y[item]:\n",
    "                distance += abs(x[item] - y[item])\n",
    "                \n",
    "        else:\n",
    "            distance += 299\n",
    "            \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lang_ngram(text, all_ngram):\n",
    "    \n",
    "    punctuation = '(\\.|,|\\?|!|\\(|\\)|\\*|\\'|\\\"|:|;|>|<|/|—|»|«|=|\\{|\\}|\\[|\\]|\\-|_|\\+|\\&|\\*|\\^|\\%|\\$|@|\\#|”)'\n",
    "    text = re.sub(punctuation, ' ', text)\n",
    "    \n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    for ngram in make_ngrams(text.replace('\\n', ' ').lower()):\n",
    "        \n",
    "        freqs[ngram] += 1\n",
    "         \n",
    "    freqs = sorted(freqs, key=lambda n: freqs[n], reverse=True)[:300]\n",
    "\n",
    "    lang_pred = defaultdict(lambda: 0)\n",
    "\n",
    "    \n",
    "    for lang in all_ngram:\n",
    "        lang_pred[lang] = distance(all_ngram[lang], freqs)\n",
    "    \n",
    "    return min(lang_pred.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ru'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_lang_ngram('что это: за! язык такой?', all_ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for lang in test_texts:\n",
    "    for text in test_texts[lang]:\n",
    "        true_labels.append(lang)\n",
    "        predicted_labels.append(predict_lang_ngram(text, all_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         be       1.00      0.99      0.99        96\n",
      "         bg       0.98      0.99      0.98        98\n",
      "         en       0.92      1.00      0.96        93\n",
      "         fr       1.00      1.00      1.00        93\n",
      "         kk       0.99      0.99      0.99       100\n",
      "         ky       0.99      0.94      0.96       100\n",
      "        lez       0.98      0.95      0.96       100\n",
      "        mhr       0.98      0.97      0.97        96\n",
      "         mk       0.98      0.99      0.99       100\n",
      "         mn       1.00      0.99      0.99       100\n",
      "         ru       0.98      0.99      0.98        95\n",
      "         uk       1.00      1.00      1.00        96\n",
      "\n",
      "avg / total       0.98      0.98      0.98      1167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94  0  0  0  1  0  0  0  0  0  0  0]\n",
      " [ 0 99  1  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 97  0  1  0  0  0  0  0  0  0]\n",
      " [ 1  2  1 94  0  0  0  0  0  1  1  0]\n",
      " [ 0  0  0  0 93  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1 99  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 93  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 96  0  0  0  0]\n",
      " [ 1  0  0  1  3  0  0  0 95  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 99  1  0]\n",
      " [ 0  0  0  0  1  0  0  0  2  0 93  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0 95]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(true_labels, predicted_labels, labels = labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из результатов, второй способ работает намного лучше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
